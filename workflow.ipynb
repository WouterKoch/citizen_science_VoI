{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing citizen scientists' contribution to automated species recognition\n",
    "This notebook executes all steps required to train a number of species recognition models based on GBIF data, and analyze the results in terms of the Value of Information of additional data.\n",
    "\n",
    "## Overview\n",
    "- Load and set settings for the setup:\n",
    "    - Tensorflow settings, directory paths\n",
    "    - Numbers of datasets, dataset sizes, splits, steps \n",
    "- Explore the data:\n",
    "    - Load a GBIF Darwin Core Archive (DwCA)\n",
    "    - Given the settings, summarize what a division per taxonomic rank would provide in terms of training data\n",
    "- Prepare the data:\n",
    "    - Based on a chosen number of species per group and taxon rank, make the groups based on the DwCA\n",
    "    - For each group, create subsets of species and observations for models to be trained on\n",
    "- Train the models:\n",
    "    - Find each job and train a model for it\n",
    "- Evaluate the models:\n",
    "    - Use the test data put aside at the data preparation step to evaluate performances\n",
    "    - Save all performance indicators to a central file\n",
    "    - Save a number of plots based on these metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dependencies\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from Tools.count_species_per_taxon import count_species_per_taxon\n",
    "\n",
    "from Scripts.voi_grouping import propse_groups, get_groups\n",
    "from Scripts.voi_create_jobs import create_jobs\n",
    "from Scripts.voi_train import train_models\n",
    "from Scripts.voi_evaluate import run_all_test_sets, collect_metrics, create_bias_plots, plot_mean_curve_per_taxon_together, plot_means_derivatives_together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings\n",
    "- Make sure you have an .env file in the root directory. See the readme for details\n",
    "- The DwCA file is assumed to contain only observations with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the env variables from the file \".env\"\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "# Settings for the generation of jobs\n",
    "dwca_file = os.path.join(os.getenv('STORAGE_DIR'), 'GBIF.zip')  # The path to the DwCA file with all data.\n",
    "train_val_test_threshold = 220  # The minimum number of observations needed per species for train + validation + test\n",
    "train_val_threshold = 200  # The minimum number of observations needed per species for train + validation (for the first, largest test set)\n",
    "validation_proportion = .1  # The proportion of observations of the train_val set reserved for validation\n",
    "reduced_factor = .75  # Every time the number of observations is reduced, it is reduced to this proportion of the previous amount\n",
    "groups = 12  # The number of taxon groups to generate and compare between\n",
    "observations_minimum = 10  # The smallest subset of observations to train models on\n",
    "duplicates = 5 # The number of independent runs per taxon group, each based on a single species subset and containing all dataset sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "This script reports the results of grouping on any taxonomic level:\n",
    "- How many groups can be made based on the threshold provided, out of the total groups within that level, and how\n",
    "many species would the smallest groups contain.\n",
    "- It also creates csv files with counts per species, and species per taxonomic group.\n",
    "\n",
    "This step provides no direct input for any following steps, and is meant to help you choose the\n",
    "settings in the next step. It does return the created species csv file for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_csv = propse_groups(dwca_file, train_val_test_threshold, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "For the dataset we used, the rank of order gives the best division, with at least 17 species per order, so this is the level we will use. Retrieve the relevant taxon groups, and create all training jobs for all duplicates for all taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_species = 17\n",
    "taxonlevel = 'order'\n",
    "\n",
    "# Create a csv file in the folder of the DwCA file with groups of the chosen level and the specified minimum number\n",
    "# of species, and store the path to that csv.\n",
    "grouping_csv = get_groups(dwca_file, train_val_test_threshold, number_of_species, taxonlevel)\n",
    "\n",
    "\n",
    "# Create jobs for each of the chose groups. This step can be repeated to generate more jobs (with different random\n",
    "# subsets of species and observations\n",
    "for i in range(duplicates):\n",
    "    create_jobs(number_of_species, train_val_threshold, reduced_factor, validation_proportion, grouping_csv,\n",
    "                species_csv, dwca_file, observations_minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Train the models defined in the job creation step. If a model exists, this will be skipped. Training will take a long time depending on your hardware, number of models and dataset sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "Evaluation requires a \"Species (total).csv\", containing a \"Taxon\" and a \"Number of species\" column (species per\n",
    "taxon group for the whole area of interest). The code below generates this file for a Norwegian context if it does\n",
    "not exist already, based on files retrieved from http://www2.artsdatabanken.no/artsnavn/Contentpages/Eksport.aspx.\n",
    "This tool is as generic as possible, expecting a folder of csv files. Some tweaking will be required for other\n",
    "contexts.\n",
    "\n",
    "All models generated in the previous step are now evaluated using the test sets separated in the data preparation step. This will take a while. When that is done, all performance and bias metrics needed will be collected and stored in .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_species_per_taxon(\n",
    "    source_folder='Artsnavnebase',\n",
    "    encoding='cp1252',\n",
    "    sep=';',\n",
    "    output_file='Species (total).csv',\n",
    "    grouping_csv=grouping_csv,\n",
    "    language='no'\n",
    ")\n",
    "\n",
    "run_all_test_sets(os.environ.get(\"JOBS_DIR\"))\n",
    "collect_metrics(os.environ.get(\"JOBS_DIR\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph creation\n",
    "A number of selected graphs will be generated for use in the manuscript. Note that some of these graphs have been post-processed manually before publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=os.path.join(os.environ.get(\"JOBS_DIR\"), \"GRAPHS\")\n",
    "path=os.environ.get(\"JOBS_DIR\")\n",
    "extension=\"pdf\"\n",
    "\n",
    "os.makedirs(os.path.join(output_path), exist_ok=True)\n",
    "\n",
    "create_bias_plots(rank=\"class\", plot_output_dir=output_path,\n",
    "                    csv_output_dir=path, extension=extension, plottype=[\"all_bias\", \"cs_img_bias\"])\n",
    "\n",
    "plot_mean_curve_per_taxon_together(path, output_path, extension, metrics=[\"F1\"])\n",
    "\n",
    "plot_means_derivatives_together(path, output_path, extension, metrics=[\"F1\"],\n",
    "                                per_species=True)\n",
    "\n",
    "create_bias_plots(rank=\"order\", plot_output_dir=output_path,\n",
    "                    csv_output_dir=path, extension=extension, plottype=\"cs_img_bias\",\n",
    "                    sortby=\"Curve derivative of F1 at average img per species\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
